{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:54:36.990594Z",
     "start_time": "2024-11-06T14:54:36.979393Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-12T13:15:09.787895Z",
     "iopub.status.busy": "2024-11-12T13:15:09.787186Z",
     "iopub.status.idle": "2024-11-12T13:15:18.879922Z",
     "shell.execute_reply": "2024-11-12T13:15:18.877497Z",
     "shell.execute_reply.started": "2024-11-12T13:15:09.787832Z"
    },
    "id": "GAC9sWkdBKGR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch 라이브러리 import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-06T14:54:45.531Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-12T13:15:18.885146Z",
     "iopub.status.busy": "2024-11-12T13:15:18.883944Z",
     "iopub.status.idle": "2024-11-12T13:15:22.091651Z",
     "shell.execute_reply": "2024-11-12T13:15:22.090793Z",
     "shell.execute_reply.started": "2024-11-12T13:15:18.885098Z"
    },
    "id": "BauBkbLmBOJM",
    "outputId": "c254d2de-0aed-4d38-c7a6-bd094eb5619a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TODO: CIFAR-100 training set 불러오기\n",
    "# import random\n",
    "\n",
    "# random.seed(2024)\n",
    "# torch.manual_seed(2024)\n",
    "# torch.cuda.manual_seed_all(2024)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='.', train=True, download=True,\n",
    "                                 transform=transform_train)\n",
    "# train_size = int(0.8 * len(train_dataset))\n",
    "# validation_size = len(train_dataset) - train_size\n",
    "# train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_size, validation_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2,\n",
    "                                           generator=torch.Generator().manual_seed(2024))\n",
    "# validation_loader = torch.utils.data.DataLoader(validation_dataset,\n",
    "#                                                batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "#            'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:52:27.661399Z",
     "start_time": "2024-11-06T14:52:27.661385Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-12T13:21:55.613771Z",
     "iopub.status.busy": "2024-11-12T13:21:55.612772Z",
     "iopub.status.idle": "2024-11-12T13:21:55.798965Z",
     "shell.execute_reply": "2024-11-12T13:21:55.798229Z",
     "shell.execute_reply.started": "2024-11-12T13:21:55.613693Z"
    },
    "id": "JngKTIlgBVhw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MLP, self).__init__()\n",
    "    torch.manual_seed(2024) # 결과 재현을 위한 seed number 고정\n",
    "    # TODO: MLP 구성 layer들 선언\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc1 = nn.Linear(3*32*32, 2048)\n",
    "    self.bn1 = nn.BatchNorm1d(2048)\n",
    "    self.fc2 = nn.Linear(2048, 512)\n",
    "    self.bn2 = nn.BatchNorm1d(512)\n",
    "    self.fc3 = nn.Linear(512, 10)\n",
    "    nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    self.relu = nn.ReLU()\n",
    "    #self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 32*32*3) # MLP가 이미지를 처리할 수 있도록 3차원 -> 1차원 벡터로 Flatten\n",
    "    # TODO: forward pass 정의\n",
    "    x = self.relu(self.bn1(self.fc1(x)))\n",
    "    #x = self.dropout(x)\n",
    "    x = self.relu(self.bn2(self.fc2(x)))\n",
    "    #x = self.dropout(x)\n",
    "    x = self.fc3(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "model = MLP().cuda()\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:54:33.979903Z",
     "start_time": "2024-11-06T14:52:27.809847Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-12T13:21:55.941456Z",
     "iopub.status.busy": "2024-11-12T13:21:55.939329Z",
     "iopub.status.idle": "2024-11-12T13:43:22.493025Z",
     "shell.execute_reply": "2024-11-12T13:43:22.491768Z",
     "shell.execute_reply.started": "2024-11-12T13:21:55.941383Z"
    },
    "id": "xO4LHsDYyXrI",
    "outputId": "fe714768-756e-4d36-adcc-0ad89951d477",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Epoch [1/50]\n",
      "     Train Loss: 1.8357\n",
      "      train_acc: 34.36%\n",
      "\n",
      "        Epoch [2/50]\n",
      "     Train Loss: 1.6495\n",
      "      train_acc: 40.43%\n",
      "\n",
      "        Epoch [3/50]\n",
      "     Train Loss: 1.5646\n",
      "      train_acc: 43.44%\n",
      "\n",
      "        Epoch [4/50]\n",
      "     Train Loss: 1.5216\n",
      "      train_acc: 45.20%\n",
      "\n",
      "        Epoch [5/50]\n",
      "     Train Loss: 1.4842\n",
      "      train_acc: 46.71%\n",
      "\n",
      "        Epoch [6/50]\n",
      "     Train Loss: 1.4556\n",
      "      train_acc: 47.70%\n",
      "\n",
      "        Epoch [7/50]\n",
      "     Train Loss: 1.4297\n",
      "      train_acc: 48.48%\n",
      "\n",
      "        Epoch [8/50]\n",
      "     Train Loss: 1.4048\n",
      "      train_acc: 49.50%\n",
      "\n",
      "        Epoch [9/50]\n",
      "     Train Loss: 1.3810\n",
      "      train_acc: 50.31%\n",
      "\n",
      "        Epoch [10/50]\n",
      "     Train Loss: 1.3606\n",
      "      train_acc: 50.90%\n",
      "\n",
      "        Epoch [11/50]\n",
      "     Train Loss: 1.3425\n",
      "      train_acc: 52.07%\n",
      "\n",
      "        Epoch [12/50]\n",
      "     Train Loss: 1.3222\n",
      "      train_acc: 52.71%\n",
      "\n",
      "        Epoch [13/50]\n",
      "     Train Loss: 1.3051\n",
      "      train_acc: 53.38%\n",
      "\n",
      "        Epoch [14/50]\n",
      "     Train Loss: 1.2883\n",
      "      train_acc: 54.02%\n",
      "\n",
      "        Epoch [15/50]\n",
      "     Train Loss: 1.2694\n",
      "      train_acc: 54.72%\n",
      "\n",
      "        Epoch [16/50]\n",
      "     Train Loss: 1.2565\n",
      "      train_acc: 55.00%\n",
      "\n",
      "        Epoch [17/50]\n",
      "     Train Loss: 1.2450\n",
      "      train_acc: 55.60%\n",
      "\n",
      "        Epoch [18/50]\n",
      "     Train Loss: 1.2255\n",
      "      train_acc: 56.16%\n",
      "\n",
      "        Epoch [19/50]\n",
      "     Train Loss: 1.2145\n",
      "      train_acc: 56.75%\n",
      "\n",
      "        Epoch [20/50]\n",
      "     Train Loss: 1.2008\n",
      "      train_acc: 57.15%\n",
      "\n",
      "        Epoch [21/50]\n",
      "     Train Loss: 1.1890\n",
      "      train_acc: 57.59%\n",
      "\n",
      "        Epoch [22/50]\n",
      "     Train Loss: 1.1748\n",
      "      train_acc: 58.10%\n",
      "\n",
      "        Epoch [23/50]\n",
      "     Train Loss: 1.1641\n",
      "      train_acc: 58.50%\n",
      "\n",
      "        Epoch [24/50]\n",
      "     Train Loss: 1.1549\n",
      "      train_acc: 58.89%\n",
      "\n",
      "        Epoch [25/50]\n",
      "     Train Loss: 1.1419\n",
      "      train_acc: 59.45%\n",
      "\n",
      "        Epoch [26/50]\n",
      "     Train Loss: 1.1341\n",
      "      train_acc: 59.77%\n",
      "\n",
      "        Epoch [27/50]\n",
      "     Train Loss: 1.1204\n",
      "      train_acc: 60.49%\n",
      "\n",
      "        Epoch [28/50]\n",
      "     Train Loss: 1.1082\n",
      "      train_acc: 60.55%\n",
      "\n",
      "        Epoch [29/50]\n",
      "     Train Loss: 1.0943\n",
      "      train_acc: 61.17%\n",
      "\n",
      "        Epoch [30/50]\n",
      "     Train Loss: 1.0886\n",
      "      train_acc: 61.33%\n",
      "\n",
      "        Epoch [31/50]\n",
      "     Train Loss: 1.0806\n",
      "      train_acc: 61.80%\n",
      "\n",
      "        Epoch [32/50]\n",
      "     Train Loss: 1.0739\n",
      "      train_acc: 61.84%\n",
      "\n",
      "        Epoch [33/50]\n",
      "     Train Loss: 1.0631\n",
      "      train_acc: 62.45%\n",
      "\n",
      "        Epoch [34/50]\n",
      "     Train Loss: 1.0506\n",
      "      train_acc: 62.69%\n",
      "\n",
      "        Epoch [35/50]\n",
      "     Train Loss: 1.0440\n",
      "      train_acc: 63.11%\n",
      "\n",
      "        Epoch [36/50]\n",
      "     Train Loss: 1.0381\n",
      "      train_acc: 63.02%\n",
      "\n",
      "        Epoch [37/50]\n",
      "     Train Loss: 1.0298\n",
      "      train_acc: 63.49%\n",
      "\n",
      "        Epoch [38/50]\n",
      "     Train Loss: 1.0250\n",
      "      train_acc: 63.59%\n",
      "\n",
      "        Epoch [39/50]\n",
      "     Train Loss: 1.0162\n",
      "      train_acc: 64.11%\n",
      "\n",
      "        Epoch [40/50]\n",
      "     Train Loss: 1.0071\n",
      "      train_acc: 64.17%\n",
      "\n",
      "        Epoch [41/50]\n",
      "     Train Loss: 1.0058\n",
      "      train_acc: 64.31%\n",
      "\n",
      "        Epoch [42/50]\n",
      "     Train Loss: 1.0036\n",
      "      train_acc: 64.62%\n",
      "\n",
      "        Epoch [43/50]\n",
      "     Train Loss: 0.9974\n",
      "      train_acc: 64.78%\n",
      "\n",
      "        Epoch [44/50]\n",
      "     Train Loss: 0.9928\n",
      "      train_acc: 64.66%\n",
      "\n",
      "        Epoch [45/50]\n",
      "     Train Loss: 0.9869\n",
      "      train_acc: 65.11%\n",
      "\n",
      "        Epoch [46/50]\n",
      "     Train Loss: 0.9878\n",
      "      train_acc: 65.08%\n",
      "\n",
      "        Epoch [47/50]\n",
      "     Train Loss: 0.9862\n",
      "      train_acc: 65.30%\n",
      "\n",
      "        Epoch [48/50]\n",
      "     Train Loss: 0.9881\n",
      "      train_acc: 65.07%\n",
      "\n",
      "        Epoch [49/50]\n",
      "     Train Loss: 0.9846\n",
      "      train_acc: 65.19%\n",
      "\n",
      "        Epoch [50/50]\n",
      "     Train Loss: 0.9853\n",
      "      train_acc: 65.21%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: model 명의로 생성된 MLP 모델에 대해 학습 수행하기\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    print(f'        Epoch [{epoch+1}/{num_epochs}]')\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100. * correct / total\n",
    "\n",
    "#     # validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_total = 0\n",
    "#     val_correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in validation_loader:\n",
    "#             images, labels = images.to('cuda'), labels.to('cuda')\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             val_total += labels.size(0)\n",
    "#             val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#     val_loss /= len(validation_loader)\n",
    "#     val_accuracy = 100. * val_correct / val_total\n",
    "\n",
    "    print(f'     Train Loss: {train_loss:.4f}')\n",
    "#     print(f'Validation Loss: {val_loss:.4f}')\n",
    "    print(f'      train_acc: {train_accuracy:.2f}%\\n')\n",
    "#     print(f'        val_acc: {val_accuracy:.2f}%\\n')\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T14:54:33.988520Z",
     "start_time": "2024-11-06T14:54:33.988507Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-12T13:43:22.495339Z",
     "iopub.status.busy": "2024-11-12T13:43:22.495002Z",
     "iopub.status.idle": "2024-11-12T13:43:24.822188Z",
     "shell.execute_reply": "2024-11-12T13:43:24.821212Z",
     "shell.execute_reply.started": "2024-11-12T13:43:22.495310Z"
    },
    "id": "lw6E8T6RBOOr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Accuracy : 63.98 %\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 평가\n",
    "test_dataset = datasets.CIFAR10(root='.', train=False, download=True,\n",
    "                                transform=transforms.ToTensor())\n",
    "batchsize = 64\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batchsize, shuffle=True)\n",
    "\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for image, label in test_loader:\n",
    "    image = image.cuda()\n",
    "    label = label.cuda()\n",
    "    output = model(image)\n",
    "    pred = output.argmax(dim=1)\n",
    "    num_correct += (pred == label).sum()\n",
    "\n",
    "print(f'Accuracy : {num_correct / len(test_dataset) * 100:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
