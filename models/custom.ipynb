{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:38:36.183053Z",
     "iopub.status.busy": "2024-11-09T13:38:36.180080Z",
     "iopub.status.idle": "2024-11-09T13:38:36.191538Z",
     "shell.execute_reply": "2024-11-09T13:38:36.190567Z",
     "shell.execute_reply.started": "2024-11-09T13:38:36.182963Z"
    },
    "id": "GAC9sWkdBKGR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch 라이브러리 import  *** 해당 cell을 수정하지 말 것 ***\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T13:54:53.260282Z",
     "iopub.status.busy": "2024-11-09T13:54:53.259680Z",
     "iopub.status.idle": "2024-11-09T13:54:54.623260Z",
     "shell.execute_reply": "2024-11-09T13:54:54.621905Z",
     "shell.execute_reply.started": "2024-11-09T13:54:53.260250Z"
    },
    "id": "BauBkbLmBOJM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# TODO: CIFAR-100 training set 불러오기\n",
    "import random\n",
    "\n",
    "random.seed(2024)\n",
    "torch.manual_seed(2024)\n",
    "torch.cuda.manual_seed_all(2024)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = datasets.CIFAR100(root='.', train=True, download=True,\n",
    "                                  transform=transform_train)\n",
    "# train_size = int(0.8 * len(train_dataset))\n",
    "# validation_size = len(train_dataset) - train_size\n",
    "# train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_size, validation_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "# validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size,\n",
    "#                                                 shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:00:58.817523Z",
     "iopub.status.busy": "2024-11-09T14:00:58.816654Z",
     "iopub.status.idle": "2024-11-09T14:00:59.440627Z",
     "shell.execute_reply": "2024-11-09T14:00:59.439555Z",
     "shell.execute_reply.started": "2024-11-09T14:00:58.817447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        torch.manual_seed(2024)  # 결과 재현을 위한 seed number 고정 *** 해당 line을 수정하지 말 것 ***\n",
    "        \n",
    "        # TODO: CNN 구성 layer들 선언\n",
    "        \n",
    "        # 32x32x3 -> 16x16x64\n",
    "        self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 16x16x64 -> 8x8x128\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 8x8x128 -> 4x4x256\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # fcl\n",
    "        self.fc4 = nn.Linear(4 * 4 * 256, 4096)\n",
    "        self.bn4 = nn.BatchNorm1d(4096)\n",
    "        self.dropout4 = nn.Dropout(0.6)\n",
    "        self.fc5 = nn.Linear(4096, 4096)\n",
    "        self.bn5 = nn.BatchNorm1d(4096)\n",
    "        self.dropout5 = nn.Dropout(0.6)\n",
    "        self.fc6 = nn.Linear(4096, 100)\n",
    "        \n",
    "        # relu\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # xavier initialization\n",
    "        nn.init.xavier_uniform_(self.conv1_1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv1_2.weight)\n",
    "        nn.init.xavier_uniform_(self.conv2_1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv2_2.weight)\n",
    "        nn.init.xavier_uniform_(self.conv3_1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv3_2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        nn.init.xavier_uniform_(self.fc5.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: forward pass 정의\n",
    "        x = self.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = self.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = self.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc6(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNN().cuda()\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:00:59.442289Z",
     "iopub.status.busy": "2024-11-09T14:00:59.441836Z",
     "iopub.status.idle": "2024-11-09T14:05:44.169993Z",
     "shell.execute_reply": "2024-11-09T14:05:44.168796Z",
     "shell.execute_reply.started": "2024-11-09T14:00:59.442261Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Epoch [1/10]\n",
      "     Train Loss: 4.5404\n",
      "      train_acc: 5.85%\n",
      "\n",
      "        Epoch [2/10]\n",
      "     Train Loss: 3.8492\n",
      "      train_acc: 14.37%\n",
      "\n",
      "        Epoch [3/10]\n",
      "     Train Loss: 3.3049\n",
      "      train_acc: 21.17%\n",
      "\n",
      "        Epoch [4/10]\n",
      "     Train Loss: 2.9016\n",
      "      train_acc: 27.22%\n",
      "\n",
      "        Epoch [5/10]\n",
      "     Train Loss: 2.6520\n",
      "      train_acc: 32.78%\n",
      "\n",
      "        Epoch [6/10]\n",
      "     Train Loss: 2.4412\n",
      "      train_acc: 36.88%\n",
      "\n",
      "        Epoch [7/10]\n",
      "     Train Loss: 2.2555\n",
      "      train_acc: 40.81%\n",
      "\n",
      "        Epoch [8/10]\n",
      "     Train Loss: 2.0589\n",
      "      train_acc: 45.33%\n",
      "\n",
      "        Epoch [9/10]\n",
      "     Train Loss: 1.8948\n",
      "      train_acc: 49.22%\n",
      "\n",
      "        Epoch [10/10]\n",
      "     Train Loss: 1.7726\n",
      "      train_acc: 52.14%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: model 명의로 생성된 CNN 모델에 대해 학습 수행하기\n",
    "for epoch in range(num_epochs):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    print(f'        Epoch [{epoch+1}/{num_epochs}]')\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100. * correct / total\n",
    "\n",
    "#     # validation\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     val_total = 0\n",
    "#     val_correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in validation_loader:\n",
    "#             images, labels = images.to('cuda'), labels.to('cuda')\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             val_total += labels.size(0)\n",
    "#             val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "#     val_loss /= len(validation_loader)\n",
    "#     val_accuracy = 100. * val_correct / val_total\n",
    "\n",
    "    print(f'     Train Loss: {train_loss:.4f}')\n",
    "    #print(f'Validation Loss: {val_loss:.4f}')\n",
    "    print(f'      train_acc: {train_accuracy:.2f}%\\n')\n",
    "    #print(f'        val_acc: {val_accuracy:.2f}%\\n')\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:05:44.174568Z",
     "iopub.status.busy": "2024-11-09T14:05:44.173926Z",
     "iopub.status.idle": "2024-11-09T14:05:46.623692Z",
     "shell.execute_reply": "2024-11-09T14:05:46.622813Z",
     "shell.execute_reply.started": "2024-11-09T14:05:44.174528Z"
    },
    "id": "lw6E8T6RBOOr",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Accuracy : 55.82 %\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 평가  *** 해당 cell을 수정하지 말 것 ***\n",
    "\n",
    "\n",
    "test_dataset = datasets.CIFAR100(root='.', train=False, download=True,\n",
    "                                 transform=transforms.ToTensor())\n",
    "batchsize = 64\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batchsize, shuffle=True)\n",
    "\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for image, label in test_loader:\n",
    "    image = image.cuda()\n",
    "    label = label.cuda()\n",
    "    output = model(image)\n",
    "    pred = output.argmax(dim=1)\n",
    "    num_correct += (pred == label).sum()\n",
    "\n",
    "print(f'Accuracy : {num_correct / len(test_dataset) * 100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
